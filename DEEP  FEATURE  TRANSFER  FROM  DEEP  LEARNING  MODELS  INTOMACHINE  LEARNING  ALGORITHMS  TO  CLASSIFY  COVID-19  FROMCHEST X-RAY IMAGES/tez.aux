\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {part}{\bf  {\onsoznameToC }}{ix}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\bf  {\contentsnameToC }}{xi}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\bf  {\kisaltmanameToC }}{xv}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\bf  {\listtablenameToC }}{xvii}{chapter*.3}\protected@file@percent }
\citation{HandMitRingen}
\citation{weight_decay_regularization}
\citation{weight_decay_regularization}
\citation{dropout_article}
\citation{A_novelCNNModel}
\citation{AlexNet}
\citation{ResNet}
\citation{parallel_linear_algebra}
\@writefile{toc}{\contentsline {part}{\bf  {\listfigurenameToC }}{xxi}{chapter*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\bf  {\abstractnameToC }}{xxiii}{chapter*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\bf  {\ozetnameToC }}{xxvii}{chapter*.4}\protected@file@percent }
\citation{history_coronavirus}
\citation{coronavirus_species}
\citation{who_pandemic_declaration}
\citation{who_qa}
\citation{covid19_news}
\citation{pcr_cleveland_clinic}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{1. \bf {INTRODUCTION}}{1}{chapter.1}\protected@file@percent }
\newlabel{CH1}{{1}{1}{INTRODUCTION}{chapter.1}{}}
\citation{covid_vs_pneumonia}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Purpose of Thesis}{2}{section.1.1}\protected@file@percent }
\newlabel{purposeofthesis}{{1.1}{2}{INTRODUCTION}{section.1.1}{}}
\citation{literature_ARDAKANI}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Literature Review}{3}{section.1.2}\protected@file@percent }
\newlabel{literaturereview}{{1.2}{3}{INTRODUCTION}{section.1.2}{}}
\citation{literature_Pathak}
\citation{imagenet}
\citation{literature_OZTURK}
\citation{yolo_darknet}
\citation{literature_oh}
\citation{literature_elshennawy}
\citation{literature_Al-Falluji}
\citation{A_novelCNNModel}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Reviewed works in the literature and their stated results.\relax }}{6}{table.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:literature_comparison}{{1.1}{6}{Reviewed works in the literature and their stated results.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure}{7}{section.1.3}\protected@file@percent }
\citation{HandMitRingen}
\citation{HandMitRingen}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{2. \bf {CHEST X-RAY}}{9}{chapter.2}\protected@file@percent }
\newlabel{ch:CH2}{{2}{9}{CHEST X-RAY}{chapter.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Hand mit Ringen (Hand with Rings): print of Wilhelm Röntgen's first "medical" X-ray, of his wife's hand \cite  {HandMitRingen}.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:first_medical_xray}{{2.1}{9}{Hand mit Ringen (Hand with Rings): print of Wilhelm Röntgen's first "medical" X-ray, of his wife's hand \cite {HandMitRingen}.\relax }{figure.caption.6}{}}
\newlabel{fig:chest_projections_a}{{2.2(a)}{10}{Subfigure 2 2.2(a)}{subfigure.2.2.1}{}}
\newlabel{sub@fig:chest_projections_a}{{(a)}{10}{Subfigure 2 2.2(a)\relax }{subfigure.2.2.1}{}}
\newlabel{fig:chest_projections_b}{{2.2(b)}{10}{Subfigure 2 2.2(b)}{subfigure.2.2.2}{}}
\newlabel{sub@fig:chest_projections_b}{{(b)}{10}{Subfigure 2 2.2(b)\relax }{subfigure.2.2.2}{}}
\newlabel{fig:chest_projections_c}{{2.2(c)}{10}{Subfigure 2 2.2(c)}{subfigure.2.2.3}{}}
\newlabel{sub@fig:chest_projections_c}{{(c)}{10}{Subfigure 2 2.2(c)\relax }{subfigure.2.2.3}{}}
\newlabel{fig:chest_projections_d}{{2.2(d)}{10}{Subfigure 2 2.2(d)}{subfigure.2.2.4}{}}
\newlabel{sub@fig:chest_projections_d}{{(d)}{10}{Subfigure 2 2.2(d)\relax }{subfigure.2.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Chest X-Ray Projections.}}{10}{figure.caption.7}\protected@file@percent }
\newlabel{chest_projections}{{2.2}{10}{Chest X-Ray Projections}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PA Projection}}}{10}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {AP Projection}}}{10}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {AP Supine Projection}}}{10}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {L Projection}}}{10}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{3. \bf {INTRODUCTION TO DEEP LEARNING}}{11}{chapter.3}\protected@file@percent }
\newlabel{ch:CH3}{{3}{11}{INTRODUCTION TO DEEP LEARNING}{chapter.3}{}}
\newlabel{fig:basic_nn_a}{{3.1(a)}{11}{Subfigure 3 3.1(a)}{subfigure.3.1.1}{}}
\newlabel{sub@fig:basic_nn_a}{{(a)}{11}{Subfigure 3 3.1(a)\relax }{subfigure.3.1.1}{}}
\newlabel{fig:basic_nn_b}{{3.1(b)}{11}{Subfigure 3 3.1(b)}{subfigure.3.1.2}{}}
\newlabel{sub@fig:basic_nn_b}{{(b)}{11}{Subfigure 3 3.1(b)\relax }{subfigure.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Sample Neuron.}}{11}{figure.caption.8}\protected@file@percent }
\newlabel{basic_neuron}{{3.1}{11}{Sample Neuron}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Neuron with two variables}}}{11}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Formula for the output of neuron on (a)}}}{11}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Basics of Deep Learning}{11}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Underfitting - Well Fitting - Overfitting.}}{13}{figure.caption.9}\protected@file@percent }
\newlabel{underfitting_overfitting}{{3.2}{13}{Underfitting - Well Fitting - Overfitting}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Cross-Entropy Loss Function}{13}{section.3.2}\protected@file@percent }
\newlabel{sec:CH3_cross_entropy}{{3.2}{13}{INTRODUCTION TO DEEP LEARNING}{section.3.2}{}}
\newlabel{eq:cross_entropy_loss_formulae}{{3.1}{13}{INTRODUCTION TO DEEP LEARNING}{equation.3.2.1}{}}
\citation{SGD_Momentum}
\citation{weight_decay_regularization}
\citation{weight_decay_regularization}
\newlabel{eq:binary_cross_entropy_loss_formulae}{{3.2}{14}{INTRODUCTION TO DEEP LEARNING}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The Basics of Optimization Methods}{14}{section.3.3}\protected@file@percent }
\newlabel{sec:CH3_the_basics_of_optimization}{{3.3}{14}{INTRODUCTION TO DEEP LEARNING}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Stochastic Gradient Descent with Momentum (SGD) Optimization Algorithm}{14}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Pseudocode for SGD with Momentum \cite  {weight_decay_regularization}.\relax }}{14}{figure.caption.10}\protected@file@percent }
\newlabel{sgd_momentum}{{3.3}{14}{Pseudocode for SGD with Momentum \cite {weight_decay_regularization}.\relax }{figure.caption.10}{}}
\citation{Adam}
\citation{Adam}
\citation{weight_decay_regularization}
\citation{weight_decay_regularization}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Adam Optimization Algorithm}{15}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Adam with Decoupled Decay (AdamW) Optimization Algorithm}{15}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Pseudocode for Adam and AdamW \cite  {weight_decay_regularization}.\relax }}{15}{figure.caption.11}\protected@file@percent }
\newlabel{adam_and_adamw}{{3.4}{15}{Pseudocode for Adam and AdamW \cite {weight_decay_regularization}.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The Basics of Convolutional Neural Networks}{15}{section.3.4}\protected@file@percent }
\newlabel{basics_of_cnn}{{3.4}{15}{INTRODUCTION TO DEEP LEARNING}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The usage example of directional derivatives and chain rule.\relax }}{16}{figure.caption.12}\protected@file@percent }
\newlabel{fig:compute_gradient}{{3.5}{16}{The usage example of directional derivatives and chain rule.\relax }{figure.caption.12}{}}
\newlabel{eq:compute_dl/dW}{{3.3}{17}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.3}{}}
\newlabel{eq:compute_dl/db}{{3.4}{17}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.4}{}}
\newlabel{eq:compute_specific_weight}{{3.5}{17}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.5}{}}
\newlabel{eq:compute_specific_bias}{{3.6}{17}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Sample CNN Architecture.}}{17}{figure.caption.13}\protected@file@percent }
\newlabel{basic_cnn_sample}{{3.6}{17}{Sample CNN Architecture}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Convolutional Layer}{17}{subsection.3.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Activation map construction by 4 x 4 filter window with stride as 2.\relax }}{19}{figure.caption.14}\protected@file@percent }
\newlabel{conv_layer}{{3.7}{19}{Activation map construction by 4 x 4 filter window with stride as 2.\relax }{figure.caption.14}{}}
\newlabel{activation_map_w_output}{{3.7}{19}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.7}{}}
\newlabel{activation_map_h_output}{{3.8}{19}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.8}{}}
\newlabel{activation_map_d_output}{{3.9}{19}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Activation Function (Rectified Linear Units - ReLU)}{19}{subsection.3.4.2}\protected@file@percent }
\citation{A_novelCNNModel}
\newlabel{eq:relu_formula}{{3.10}{20}{INTRODUCTION TO DEEP LEARNING}{equation.3.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Pooling Layer}{20}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Maximum pooling operation sample with 2 x 2 window and stride as 2.}}{20}{figure.caption.15}\protected@file@percent }
\newlabel{fig:maxpooling}{{3.8}{20}{Maximum pooling operation sample with 2 x 2 window and stride as 2}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Batch Normalization}{20}{subsection.3.4.4}\protected@file@percent }
\citation{dropout_article}
\citation{dropout_article}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Dropout}{21}{subsection.3.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces (a) A standard neural network, (b) A neural network after applying dropout\cite  {dropout_article}.\relax }}{21}{figure.caption.16}\protected@file@percent }
\newlabel{fig:dropout}{{3.9}{21}{(a) A standard neural network, (b) A neural network after applying dropout\cite {dropout_article}.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Flattening}{21}{subsection.3.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Fully-Connected Layer}{21}{subsection.3.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Transfer Learning}{21}{section.3.5}\protected@file@percent }
\citation{imagenet}
\citation{A_novelCNNModel}
\citation{A_novelCNNModel}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The usage of full-connected layer for a binary classifier.}}{22}{figure.caption.17}\protected@file@percent }
\newlabel{fig:fully_connected_layer}{{3.10}{22}{The usage of full-connected layer for a binary classifier}{figure.caption.17}{}}
\citation{AlexNet}
\citation{imagenet}
\citation{AlexNet}
\citation{AlexNet}
\citation{ResNet}
\citation{imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Different fine-tuning strategies on pre-trained model.}}{23}{figure.caption.18}\protected@file@percent }
\newlabel{fig:pretrain_strategies}{{3.11}{23}{Different fine-tuning strategies on pre-trained model}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}CNN Models}{23}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}AlexNet}{23}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Residual Neural Network (ResNet)}{23}{subsection.3.6.2}\protected@file@percent }
\citation{ResNet}
\citation{ResNet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Deep feature extraction from CNN models and using them in machine learning models \cite  {A_novelCNNModel}.\relax }}{24}{figure.caption.19}\protected@file@percent }
\newlabel{fig:A_novelCNNModel_architecture}{{3.12}{24}{Deep feature extraction from CNN models and using them in machine learning models \cite {A_novelCNNModel}.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {ResNet-18}}{24}{figure.caption.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces An illustration of AlexNet architecture with input image in the size of 224 × 224 × 3 \cite  {AlexNet}.\relax }}{25}{figure.caption.20}\protected@file@percent }
\newlabel{fig:alexnet_arch}{{3.13}{25}{An illustration of AlexNet architecture with input image in the size of 224 × 224 × 3 \cite {AlexNet}.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces An illustration for residual block. Here, $a^{[l]}$ is the starting activation layer and $a^{[l+2]}$ is the fed activation layer.}}{25}{figure.caption.21}\protected@file@percent }
\newlabel{fig:residual_block}{{3.14}{25}{An illustration for residual block. Here, $a^{[l]}$ is the starting activation layer and $a^{[l+2]}$ is the fed activation layer}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {ResNet-34}}{25}{figure.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\textit  {ResNet-50}}{25}{figure.caption.22}\protected@file@percent }
\citation{VGG}
\citation{imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The ResNet architectures\cite  {ResNet}.\relax }}{26}{figure.caption.22}\protected@file@percent }
\newlabel{fig:resnet_archs}{{3.15}{26}{The ResNet architectures\cite {ResNet}.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}VGG}{26}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\textit  {VGG16}}{26}{figure.caption.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces An illustration of VGG architectures.}}{27}{figure.caption.23}\protected@file@percent }
\newlabel{fig:vgg16_vgg19_archs}{{3.16}{27}{An illustration of VGG architectures}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {VGG19}}{27}{figure.caption.23}\protected@file@percent }
\citation{parallel_linear_algebra}
\citation{parallel_linear_algebra}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{4. \bf {INTRODUCTION TO MACHINE LEARNING}}{29}{chapter.4}\protected@file@percent }
\newlabel{ch:CH4}{{4}{29}{INTRODUCTION TO MACHINE LEARNING}{chapter.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Sample Machine Learning Road Map.\relax }}{29}{figure.caption.24}\protected@file@percent }
\newlabel{basic_ml}{{4.1}{29}{Sample Machine Learning Road Map.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The Basics of Machine Learning (ML)}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Supervised Learning}{29}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces (a) Clustering Problem, (b) Classification Problem, and (c) Regression Problem \cite  {parallel_linear_algebra}.\relax }}{30}{figure.caption.25}\protected@file@percent }
\newlabel{clustering_classification_regression}{{4.2}{30}{(a) Clustering Problem, (b) Classification Problem, and (c) Regression Problem \cite {parallel_linear_algebra}.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Unsupervised Learning}{30}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Cross-Validation (CV)}{30}{section.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces K-Fold Cross-Validation.}}{31}{figure.caption.26}\protected@file@percent }
\newlabel{k_fold_cv}{{4.3}{31}{K-Fold Cross-Validation}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}K-Fold CV}{31}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Leave-One-Out (LOO)}{31}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Regularization}{32}{section.4.3}\protected@file@percent }
\newlabel{sec:CH5_regularization}{{4.3}{32}{INTRODUCTION TO MACHINE LEARNING}{section.4.3}{}}
\newlabel{eq:general_regularized_loss}{{4.1}{32}{INTRODUCTION TO MACHINE LEARNING}{equation.4.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}L1 Regularization (LASSO)}{32}{subsection.4.3.1}\protected@file@percent }
\newlabel{eq:lasso_term}{{4.2}{32}{INTRODUCTION TO MACHINE LEARNING}{equation.4.3.2}{}}
\citation{lasso_penalty}
\citation{svm_original}
\citation{support_vector_clustering}
\newlabel{eq:l1_norm}{{4.3}{33}{INTRODUCTION TO MACHINE LEARNING}{equation.4.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}L2 Regularization (Ridge)}{33}{subsection.4.3.2}\protected@file@percent }
\newlabel{eq:ridge_term}{{4.4}{33}{INTRODUCTION TO MACHINE LEARNING}{equation.4.3.4}{}}
\newlabel{eq:l2_norm}{{4.5}{33}{INTRODUCTION TO MACHINE LEARNING}{equation.4.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}ML Models}{33}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Support-Vector Machines (SVM)}{33}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces A Hyper-plane and its margins for an SVM to a binary classification problem.}}{34}{figure.caption.27}\protected@file@percent }
\newlabel{simple_svm}{{4.4}{34}{A Hyper-plane and its margins for an SVM to a binary classification problem}{figure.caption.27}{}}
\newlabel{eq:linear_hyperplane}{{4.6}{34}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Loss Functions}}{34}{equation.4.4.6}\protected@file@percent }
\newlabel{h0_formula}{{4.7}{34}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.7}{}}
\newlabel{h+1_formula}{{4.8}{35}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.8}{}}
\newlabel{h-1_formula}{{4.9}{35}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.9}{}}
\newlabel{common_loss}{{4.10}{35}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.10}{}}
\newlabel{hinge_loss}{{4.11}{35}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.11}{}}
\newlabel{squared_hinge_loss}{{4.12}{35}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.12}{}}
\newlabel{eq:primal_svm}{{4.13}{35}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.13}{}}
\citation{svm_book}
\citation{svm_penalty}
\citation{svm_penalty}
\newlabel{eq:dual_svm}{{4.14}{36}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Penalty Terms}}{36}{equation.4.4.14}\protected@file@percent }
\newlabel{eq:l2_svm_primal}{{4.15}{36}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.15}{}}
\newlabel{eq:l2_svm_dual}{{4.16}{36}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.16}{}}
\newlabel{eq:l1_svm_primal}{{4.17}{36}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.17}{}}
\newlabel{eq:l1_svm_dual}{{4.18}{36}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Kernel Trick}}{37}{equation.4.4.18}\protected@file@percent }
\newlabel{eq:kernel_function}{{4.19}{37}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.19}{}}
\newlabel{eq:lienar_kernel_function}{{4.20}{37}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.20}{}}
\newlabel{eq:rbf_kernel_function}{{4.21}{37}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.21}{}}
\newlabel{sigmoid_kernel_function}{{4.22}{37}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Logistic Regression (LR)}{37}{subsection.4.4.2}\protected@file@percent }
\newlabel{eq:logistic_p}{{4.23}{37}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Logistic sigmoid function with different scaling w values. \relax }}{38}{figure.caption.28}\protected@file@percent }
\newlabel{logistic_function}{{4.5}{38}{Logistic sigmoid function with different scaling w values. \relax }{figure.caption.28}{}}
\newlabel{eq:logistic_hyptothesis}{{4.24}{38}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.24}{}}
\newlabel{log_loss_onesample}{{4.25}{38}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.25}{}}
\newlabel{lr_optimization}{{4.26}{38}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Penalty Terms}}{38}{equation.4.4.26}\protected@file@percent }
\citation{scikit-learn}
\citation{lr_newton_cg}
\citation{lr_lbfgsb}
\citation{lr_liblinear}
\citation{lr_sag}
\citation{lr_saga}
\citation{knn_pdf}
\newlabel{lr_l1}{{4.27}{39}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.27}{}}
\newlabel{lr_l1}{{4.28}{39}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Optimizers}}{39}{equation.4.4.28}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Logistic Regression optimization problem solvers.\relax }}{39}{table.caption.29}\protected@file@percent }
\newlabel{tab:lr_solver_table}{{4.1}{39}{Logistic Regression optimization problem solvers.\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}K-Nearest Neighbor (KNN)}{39}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Algorithm}}{40}{section*.30}\protected@file@percent }
\newlabel{eucledian_distance}{{4.29}{40}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.29}{}}
\newlabel{manhattan_distance}{{4.30}{40}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.30}{}}
\newlabel{chebyshev_distance}{{4.31}{40}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.31}{}}
\citation{scikit-learn}
\citation{kd_tree}
\citation{ball_tree}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces KNN example illustration to detect the class for the green sample in the middle.}}{41}{figure.caption.31}\protected@file@percent }
\newlabel{knn_example}{{4.6}{41}{KNN example illustration to detect the class for the green sample in the middle}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Finding Neighborhood}}{41}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Linear Discriminant Analysis (LDA)}{42}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Derivation}}{42}{section*.32}\protected@file@percent }
\newlabel{eq:prob_x_to_ci}{{4.32}{42}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.32}{}}
\newlabel{eq:log_g}{{4.33}{42}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.33}{}}
\newlabel{eq:normal_distribution}{{4.34}{42}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.34}{}}
\newlabel{eq:prior_linear_disc_func}{{4.35}{42}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.35}{}}
\citation{regularized_lda}
\newlabel{eq:linear_disc_func}{{4.36}{43}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.36}{}}
\newlabel{linear_disc_func_case1}{{4.37}{43}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.37}{}}
\newlabel{linear_disc_func_case2}{{4.38}{43}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces LDA process for fixed and varying covariances.}}{44}{figure.caption.33}\protected@file@percent }
\newlabel{lda_example}{{4.7}{44}{LDA process for fixed and varying covariances}{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Penalty Terms}}{44}{figure.caption.33}\protected@file@percent }
\newlabel{cov_regularization}{{4.39}{44}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.39}{}}
\citation{scikit-learn}
\citation{scikit-learn_lda-solvers}
\newlabel{eq:regularized_linear_disc_func}{{4.42}{45}{INTRODUCTION TO MACHINE LEARNING}{equation.4.4.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{\textit  {Solvers}}{45}{equation.4.4.42}\protected@file@percent }
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{5. \bf {EXPERIMENTS}}{47}{chapter.5}\protected@file@percent }
\newlabel{ch:CH5}{{5}{47}{EXPERIMENTS}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Dataset}{47}{section.5.1}\protected@file@percent }
\newlabel{fig:dataset_samples_a}{{5.1(a)}{48}{Subfigure 5 5.1(a)}{subfigure.5.1.1}{}}
\newlabel{sub@fig:dataset_samples_a}{{(a)}{48}{Subfigure 5 5.1(a)\relax }{subfigure.5.1.1}{}}
\newlabel{fig:dataset_samples_b}{{5.1(b)}{48}{Subfigure 5 5.1(b)}{subfigure.5.1.2}{}}
\newlabel{sub@fig:dataset_samples_b}{{(b)}{48}{Subfigure 5 5.1(b)\relax }{subfigure.5.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Dataset Samples.\relax }}{48}{figure.caption.34}\protected@file@percent }
\newlabel{dataset_samples}{{5.1}{48}{Dataset Samples.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Train COVID-19 Data}}}{48}{figure.caption.34}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test non-COVID-19 Data}}}{48}{figure.caption.34}\protected@file@percent }
\newlabel{fig:data_transform_steps_a}{{5.2(a)}{48}{Subfigure 5 5.2(a)}{subfigure.5.2.1}{}}
\newlabel{sub@fig:data_transform_steps_a}{{(a)}{48}{Subfigure 5 5.2(a)\relax }{subfigure.5.2.1}{}}
\newlabel{fig:data_transform_steps_b}{{5.2(b)}{48}{Subfigure 5 5.2(b)}{subfigure.5.2.2}{}}
\newlabel{sub@fig:data_transform_steps_b}{{(b)}{48}{Subfigure 5 5.2(b)\relax }{subfigure.5.2.2}{}}
\newlabel{fig:data_transform_steps_c}{{5.2(c)}{48}{Subfigure 5 5.2(c)}{subfigure.5.2.3}{}}
\newlabel{sub@fig:data_transform_steps_c}{{(c)}{48}{Subfigure 5 5.2(c)\relax }{subfigure.5.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Image Transformation Steps - Test COVID-19 Sample.\relax }}{48}{figure.caption.35}\protected@file@percent }
\newlabel{data_transform_steps}{{5.2}{48}{Image Transformation Steps - Test COVID-19 Sample.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original (826×768)}}}{48}{figure.caption.35}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Resized, Square Cropped and Gray-Scaled}}}{48}{figure.caption.35}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Normalized}}}{48}{figure.caption.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Data Augmentation}{48}{section.5.2}\protected@file@percent }
\citation{imagenet}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces The detailed number of data.\relax }}{49}{table.caption.36}\protected@file@percent }
\newlabel{tab:final_dataset_size}{{5.1}{49}{The detailed number of data.\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Training and Testing on Convolutional Neural Network Models}{49}{section.5.3}\protected@file@percent }
\newlabel{sec:CH5_cnn_experiments}{{5.3}{49}{EXPERIMENTS}{section.5.3}{}}
\newlabel{fig:augmented_sample_a}{{5.3(a)}{50}{Subfigure 5 5.3(a)}{subfigure.5.3.1}{}}
\newlabel{sub@fig:augmented_sample_a}{{(a)}{50}{Subfigure 5 5.3(a)\relax }{subfigure.5.3.1}{}}
\newlabel{fig:augmented_sample_b}{{5.3(b)}{50}{Subfigure 5 5.3(b)}{subfigure.5.3.2}{}}
\newlabel{sub@fig:augmented_sample_b}{{(b)}{50}{Subfigure 5 5.3(b)\relax }{subfigure.5.3.2}{}}
\newlabel{fig:augmented_sample_c}{{5.3(c)}{50}{Subfigure 5 5.3(c)}{subfigure.5.3.3}{}}
\newlabel{sub@fig:augmented_sample_c}{{(c)}{50}{Subfigure 5 5.3(c)\relax }{subfigure.5.3.3}{}}
\newlabel{fig:augmented_sample_d}{{5.3(d)}{50}{Subfigure 5 5.3(d)}{subfigure.5.3.4}{}}
\newlabel{sub@fig:augmented_sample_d}{{(d)}{50}{Subfigure 5 5.3(d)\relax }{subfigure.5.3.4}{}}
\newlabel{fig:augmented_sample_e}{{5.3(e)}{50}{Subfigure 5 5.3(e)}{subfigure.5.3.5}{}}
\newlabel{sub@fig:augmented_sample_e}{{(e)}{50}{Subfigure 5 5.3(e)\relax }{subfigure.5.3.5}{}}
\newlabel{fig:augmented_sample_f}{{5.3(f)}{50}{Subfigure 5 5.3(f)}{subfigure.5.3.6}{}}
\newlabel{sub@fig:augmented_sample_f}{{(f)}{50}{Subfigure 5 5.3(f)\relax }{subfigure.5.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Original Sample and Augmentation.\relax }}{50}{figure.caption.37}\protected@file@percent }
\newlabel{augmented_sample}{{5.3}{50}{Original Sample and Augmentation.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original Sample - Train non-COVID-19}}}{50}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Horizontal Flipped}}}{50}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Horizontal Flipped}}}{50}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Horizontal Flipped}}}{50}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Horizontal Flipped}}}{50}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Horizontal Flipped}}}{50}{figure.caption.37}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces CNN models final hyper-parameters.\relax }}{51}{table.caption.38}\protected@file@percent }
\newlabel{tab:cnn_hyperparameters}{{5.2}{51}{CNN models final hyper-parameters.\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The sample visualization representing one iteration on ResNet-50 architecture and class probability result for COVID-19 labeled data from our dataset. The higher resolution version of image is available at   \href  {https://github.com/ozanguldali/modelsWithLASSO/blob/master/figures/resnet50_visual.png}{\url  {https://github.com/ozanguldali/modelsWithLASSO/blob/master/figures/resnet50_visual.png}}\relax }}{53}{figure.caption.39}\protected@file@percent }
\newlabel{fig:resnet50_visualization}{{5.4}{53}{The sample visualization representing one iteration on ResNet-50 architecture and class probability result for COVID-19 labeled data from our dataset. The higher resolution version of image is available at \\ \hyperrefurl {https://github.com/ozanguldali/modelsWithLASSO/blob/master/figures/resnet50_visual.png}\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Deep Feature Extraction}{54}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Modified classification blocks where bold red windowed fully-connected layers denotes the deep feature extraction state.\relax }}{54}{figure.caption.40}\protected@file@percent }
\newlabel{fig:modified_classification_blocks}{{5.5}{54}{Modified classification blocks where bold red windowed fully-connected layers denotes the deep feature extraction state.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Forming the Feature Maps}{55}{section.5.5}\protected@file@percent }
\newlabel{sec:CH5_forming_features}{{5.5}{55}{EXPERIMENTS}{section.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Feature maps where $d_{cnn} = 1000$, $d_{info} = 2$, and $d_{all} = d_{cnn} + d_{info} = 1002$.\relax }}{55}{figure.caption.41}\protected@file@percent }
\newlabel{fig:feature_maps}{{5.6}{55}{Feature maps where $d_{cnn} = 1000$, $d_{info} = 2$, and $d_{all} = d_{cnn} + d_{info} = 1002$.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Data Pre-Processing}{55}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Data Standardization}{55}{subsection.5.6.1}\protected@file@percent }
\newlabel{eq:data_standardization}{{5.1}{56}{EXPERIMENTS}{equation.5.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Data Normalization}{56}{subsection.5.6.2}\protected@file@percent }
\newlabel{eq:data_l1_norm}{{5.2}{56}{EXPERIMENTS}{equation.5.6.2}{}}
\newlabel{eq:data_l2_norm}{{5.3}{56}{EXPERIMENTS}{equation.5.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Hyper-Parameter Tuning}{56}{section.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Training and Testing on Machine Learning Models}{57}{section.5.8}\protected@file@percent }
\citation{TULIP_package}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces SVM algorithm hyper-parameters to tune.\relax }}{58}{table.caption.42}\protected@file@percent }
\newlabel{tab:svm_hyperparameter_table}{{5.3}{58}{SVM algorithm hyper-parameters to tune.\relax }{table.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces LR algorithm hyper-parameters to tune.\relax }}{58}{table.caption.43}\protected@file@percent }
\newlabel{tab:lr_hyperparameter_table}{{5.4}{58}{LR algorithm hyper-parameters to tune.\relax }{table.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces KNN algorithm hyper-parameters to tune.\relax }}{58}{table.caption.44}\protected@file@percent }
\newlabel{tab:knn_hyperparameter_table}{{5.5}{58}{KNN algorithm hyper-parameters to tune.\relax }{table.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces LDA algorithm hyper-parameters to tune.\relax }}{59}{table.caption.45}\protected@file@percent }
\newlabel{tab:lda_hyperparameter_table}{{5.6}{59}{LDA algorithm hyper-parameters to tune.\relax }{table.caption.45}{}}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{6. \bf {RESULTS}}{61}{chapter.6}\protected@file@percent }
\newlabel{ch:CH6}{{6}{61}{RESULTS}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Performance Measurement}{61}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Confusion Matrix}{61}{subsection.6.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Sample Confusion Matrix.\relax }}{61}{table.caption.46}\protected@file@percent }
\newlabel{sample_confusion_matrix}{{6.1}{61}{Sample Confusion Matrix.\relax }{table.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}The Analysis of Confusion Matrix}{62}{subsection.6.1.2}\protected@file@percent }
\newlabel{eq:weighted_avg_metric}{{6.1}{62}{RESULTS}{equation.6.1.1}{}}
\newlabel{eq:weighted_avg_binary_class_metric}{{6.2}{62}{RESULTS}{equation.6.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2.1}Sensitivity (True Positive Rate - TPR)}{62}{subsubsection.6.1.2.1}\protected@file@percent }
\newlabel{eq:TPR}{{6.3}{62}{RESULTS}{equation.6.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2.2}Specificity (True Negative Rate - TNR)}{62}{subsubsection.6.1.2.2}\protected@file@percent }
\newlabel{eq:TNR}{{6.4}{62}{RESULTS}{equation.6.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2.3}Precision (Positive Predictive Value - PPV)}{63}{subsubsection.6.1.2.3}\protected@file@percent }
\newlabel{eq:PPV}{{6.5}{63}{RESULTS}{equation.6.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2.4}Accuracy (ACC)}{63}{subsubsection.6.1.2.4}\protected@file@percent }
\newlabel{eq:ACC}{{6.6}{63}{RESULTS}{equation.6.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2.5}F1 Score}{63}{subsubsection.6.1.2.5}\protected@file@percent }
\newlabel{eq:F1_Score}{{6.7}{63}{RESULTS}{equation.6.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2.6}Area Under the Curve (AUC Score)}{63}{subsubsection.6.1.2.6}\protected@file@percent }
\newlabel{eq:AUC_Score}{{6.8}{63}{RESULTS}{equation.6.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Convolutional Neural Network Results}{63}{section.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces AlexNet model accuracy and loss curves for three different optimizers on train and validation processes.\relax }}{65}{figure.caption.47}\protected@file@percent }
\newlabel{fig:alexnet_plots}{{6.1}{65}{AlexNet model accuracy and loss curves for three different optimizers on train and validation processes.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces ResNet-18 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }}{66}{figure.caption.48}\protected@file@percent }
\newlabel{fig:resnet18_plots}{{6.2}{66}{ResNet-18 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces ResNet-34 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }}{67}{figure.caption.49}\protected@file@percent }
\newlabel{fig:resnet34_plots}{{6.3}{67}{ResNet-34 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces ResNet-50 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }}{68}{figure.caption.50}\protected@file@percent }
\newlabel{fig:resnet50_plots}{{6.4}{68}{ResNet-50 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces VGG16 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }}{69}{figure.caption.51}\protected@file@percent }
\newlabel{fig:vgg16_plots}{{6.5}{69}{VGG16 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces VGG19 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }}{70}{figure.caption.52}\protected@file@percent }
\newlabel{fig:vgg19_plots}{{6.6}{70}{VGG19 model accuracy and loss curves for three different optimizers on train and validation processes.\relax }{figure.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces CNN model comparison.\relax }}{71}{table.caption.53}\protected@file@percent }
\newlabel{tab:cnn_result_table}{{6.2}{71}{CNN model comparison.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces The confusion matrix of AlexNet model result with Adam optimizer.\relax }}{72}{table.caption.54}\protected@file@percent }
\newlabel{tab:conf_alexnet}{{6.3}{72}{The confusion matrix of AlexNet model result with Adam optimizer.\relax }{table.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces The confusion matrix of ResNet-18 model result with AdamW optimizer.\relax }}{72}{table.caption.55}\protected@file@percent }
\newlabel{tab:conf_resnet18}{{6.4}{72}{The confusion matrix of ResNet-18 model result with AdamW optimizer.\relax }{table.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces The confusion matrix of ResNet-34 model result with AdamW optimizer.\relax }}{72}{table.caption.56}\protected@file@percent }
\newlabel{tab:conf_resnet34}{{6.5}{72}{The confusion matrix of ResNet-34 model result with AdamW optimizer.\relax }{table.caption.56}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces The confusion matrix of ResNet-50 model result with Adam optimizer.\relax }}{72}{table.caption.57}\protected@file@percent }
\newlabel{tab:conf_resnet50}{{6.6}{72}{The confusion matrix of ResNet-50 model result with Adam optimizer.\relax }{table.caption.57}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces The confusion matrix of VGG16 model result with AdamW optimizer.\relax }}{72}{table.caption.58}\protected@file@percent }
\newlabel{tab:conf_vgg16}{{6.7}{72}{The confusion matrix of VGG16 model result with AdamW optimizer.\relax }{table.caption.58}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces The confusion matrix of VGG19 model result with SGD Momentum optimizer.\relax }}{72}{table.caption.59}\protected@file@percent }
\newlabel{tab:conf_vgg19}{{6.8}{72}{The confusion matrix of VGG19 model result with SGD Momentum optimizer.\relax }{table.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Machine Learning Results}{73}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Results for $X_{info}$}{73}{subsection.6.3.1}\protected@file@percent }
\newlabel{CH6:results_xinfo}{{6.3.1}{73}{RESULTS}{subsection.6.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.9}{\ignorespaces The result of demographic information experiments with the Python system seed as 4.\relax }}{73}{table.caption.60}\protected@file@percent }
\newlabel{tab:ml_info_result_table}{{6.9}{73}{The result of demographic information experiments with the Python system seed as 4.\relax }{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.10}{\ignorespaces KNN algorithm hyper-parameters for $X_{info}$.\relax }}{73}{table.caption.61}\protected@file@percent }
\newlabel{tab:knn_info_params}{{6.10}{73}{KNN algorithm hyper-parameters for $X_{info}$.\relax }{table.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Results for $X_{cnn}$}{73}{subsection.6.3.2}\protected@file@percent }
\newlabel{CH6:results_xcnn}{{6.3.2}{73}{RESULTS}{subsection.6.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.11}{\ignorespaces Confusion matrices for KNN experiments on demographic information.\relax }}{74}{table.caption.62}\protected@file@percent }
\newlabel{tab:knn_conf_matrix_xinfo}{{6.11}{74}{Confusion matrices for KNN experiments on demographic information.\relax }{table.caption.62}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.12}{\ignorespaces The result of experiments on $X_{cnn}$ with the Python system seed as 4.\relax }}{75}{table.caption.63}\protected@file@percent }
\newlabel{tab:ml_cnn_result_table}{{6.12}{75}{The result of experiments on $X_{cnn}$ with the Python system seed as 4.\relax }{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.13}{\ignorespaces SVM algorithm regularized by Ridge hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{76}{table.caption.64}\protected@file@percent }
\newlabel{tab:svmridge_cnn_params}{{6.13}{76}{SVM algorithm regularized by Ridge hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.14}{\ignorespaces SVM algorithm regularized by Lasso hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{76}{table.caption.65}\protected@file@percent }
\newlabel{tab:svmlasso_cnn_params}{{6.14}{76}{SVM algorithm regularized by Lasso hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.65}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.15}{\ignorespaces LR algorithm hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{76}{table.caption.66}\protected@file@percent }
\newlabel{tab:lr_cnn_params}{{6.15}{76}{LR algorithm hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.66}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.16}{\ignorespaces LR algorithm regularized by Ridge hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{76}{table.caption.67}\protected@file@percent }
\newlabel{tab:lrridge_cnn_params}{{6.16}{76}{LR algorithm regularized by Ridge hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.67}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.17}{\ignorespaces LR algorithm regularized by Lasso hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{76}{table.caption.68}\protected@file@percent }
\newlabel{tab:lrlasso_cnn_params}{{6.17}{76}{LR algorithm regularized by Lasso hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.18}{\ignorespaces KNN algorithm hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{76}{table.caption.69}\protected@file@percent }
\newlabel{tab:knn_cnn_params}{{6.18}{76}{KNN algorithm hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.69}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.19}{\ignorespaces LDA algorithm hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{77}{table.caption.70}\protected@file@percent }
\newlabel{tab:lda_cnn_params}{{6.19}{77}{LDA algorithm hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.70}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.20}{\ignorespaces LDA algorithm regularized by Lasso hyper-parameters for $X_{cnn\_ResNet50}$.\relax }}{77}{table.caption.71}\protected@file@percent }
\newlabel{tab:ldalasso_cnn_params}{{6.20}{77}{LDA algorithm regularized by Lasso hyper-parameters for $X_{cnn\_ResNet50}$.\relax }{table.caption.71}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.21}{\ignorespaces The confusion matrices and results obtained by SVM algorithm regularized with Ridge for $X_{cnn\_ResNet50}$.\relax }}{78}{table.caption.72}\protected@file@percent }
\newlabel{tab:svmridge_cnn_results}{{6.21}{78}{The confusion matrices and results obtained by SVM algorithm regularized with Ridge for $X_{cnn\_ResNet50}$.\relax }{table.caption.72}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.22}{\ignorespaces The confusion matrices and results obtained by SVM algorithm regularized with Lasso for $X_{cnn\_ResNet50}$.\relax }}{78}{table.caption.73}\protected@file@percent }
\newlabel{tab:svmlasso_cnn_results}{{6.22}{78}{The confusion matrices and results obtained by SVM algorithm regularized with Lasso for $X_{cnn\_ResNet50}$.\relax }{table.caption.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.23}{\ignorespaces The confusion matrices and results obtained by LR algorithm for $X_{cnn\_ResNet50}$.\relax }}{78}{table.caption.74}\protected@file@percent }
\newlabel{tab:lr_cnn_results}{{6.23}{78}{The confusion matrices and results obtained by LR algorithm for $X_{cnn\_ResNet50}$.\relax }{table.caption.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.24}{\ignorespaces The confusion matrices and results obtained by LR algorithm regularized with Ridge for $X_{cnn\_ResNet50}$.\relax }}{79}{table.caption.75}\protected@file@percent }
\newlabel{tab:lrridge_cnn_results}{{6.24}{79}{The confusion matrices and results obtained by LR algorithm regularized with Ridge for $X_{cnn\_ResNet50}$.\relax }{table.caption.75}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.25}{\ignorespaces The confusion matrices and results obtained by LR algorithm regularized with Lasso for $X_{cnn\_ResNet50}$.\relax }}{79}{table.caption.76}\protected@file@percent }
\newlabel{tab:lrlasso_cnn_results}{{6.25}{79}{The confusion matrices and results obtained by LR algorithm regularized with Lasso for $X_{cnn\_ResNet50}$.\relax }{table.caption.76}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.26}{\ignorespaces The confusion matrices and results obtained by KNN algorithm for $X_{cnn\_ResNet50}$.\relax }}{79}{table.caption.77}\protected@file@percent }
\newlabel{tab:knn_cnn_results}{{6.26}{79}{The confusion matrices and results obtained by KNN algorithm for $X_{cnn\_ResNet50}$.\relax }{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.27}{\ignorespaces The confusion matrices and results obtained by LDA algorithm for $X_{cnn\_ResNet50}$.\relax }}{79}{table.caption.78}\protected@file@percent }
\newlabel{tab:lda_cnn_results}{{6.27}{79}{The confusion matrices and results obtained by LDA algorithm for $X_{cnn\_ResNet50}$.\relax }{table.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.28}{\ignorespaces The confusion matrices and results obtained by LDA algorithm regularized with Lasso for $X_{cnn\_ResNet50}$.\relax }}{80}{table.caption.79}\protected@file@percent }
\newlabel{tab:ldalasso_cnn_results}{{6.28}{80}{The confusion matrices and results obtained by LDA algorithm regularized with Lasso for $X_{cnn\_ResNet50}$.\relax }{table.caption.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Results for $X_{all}$}{81}{subsection.6.3.3}\protected@file@percent }
\newlabel{CH6:results_xall}{{6.3.3}{81}{RESULTS}{subsection.6.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.29}{\ignorespaces The result of experiments on $X_{all}$ with the Python system seed as 4.\relax }}{82}{table.caption.80}\protected@file@percent }
\newlabel{tab:ml_all_result_table}{{6.29}{82}{The result of experiments on $X_{all}$ with the Python system seed as 4.\relax }{table.caption.80}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.30}{\ignorespaces SVM algorithm regularized by Ridge hyper-parameters for $X_{all\_ResNet50}$.\relax }}{83}{table.caption.81}\protected@file@percent }
\newlabel{tab:svmridge_all_params}{{6.30}{83}{SVM algorithm regularized by Ridge hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.31}{\ignorespaces SVM algorithm regularized by Lasso hyper-parameters for $X_{all\_ResNet50}$.\relax }}{83}{table.caption.82}\protected@file@percent }
\newlabel{tab:svmlasso_all_params}{{6.31}{83}{SVM algorithm regularized by Lasso hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.82}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.32}{\ignorespaces LR algorithm hyper-parameters for $X_{all\_ResNet50}$.\relax }}{83}{table.caption.83}\protected@file@percent }
\newlabel{tab:lr_all_params}{{6.32}{83}{LR algorithm hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.83}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.33}{\ignorespaces LR algorithm regularized by Ridge hyper-parameters for $X_{all\_ResNet50}$.\relax }}{83}{table.caption.84}\protected@file@percent }
\newlabel{tab:lrridge_all_params}{{6.33}{83}{LR algorithm regularized by Ridge hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.34}{\ignorespaces LR algorithm regularized by Lasso hyper-parameters for $X_{all\_ResNet50}$.\relax }}{83}{table.caption.85}\protected@file@percent }
\newlabel{tab:lrlasso_all_params}{{6.34}{83}{LR algorithm regularized by Lasso hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.85}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.35}{\ignorespaces KNN algorithm hyper-parameters for $X_{all\_ResNet50}$.\relax }}{83}{table.caption.86}\protected@file@percent }
\newlabel{tab:knn_all_params}{{6.35}{83}{KNN algorithm hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.36}{\ignorespaces LDA algorithm hyper-parameters for $X_{all\_ResNet50}$.\relax }}{84}{table.caption.87}\protected@file@percent }
\newlabel{tab:lda_all_params}{{6.36}{84}{LDA algorithm hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.87}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.37}{\ignorespaces LDA algorithm regularized by Lasso hyper-parameters for $X_{all\_ResNet50}$.\relax }}{84}{table.caption.88}\protected@file@percent }
\newlabel{tab:ldalasso_all_params}{{6.37}{84}{LDA algorithm regularized by Lasso hyper-parameters for $X_{all\_ResNet50}$.\relax }{table.caption.88}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.38}{\ignorespaces The confusion matrices and results obtained by SVM algorithm regularized with Ridge for $X_{all\_ResNet50}$.\relax }}{85}{table.caption.89}\protected@file@percent }
\newlabel{tab:svmridge_all_results}{{6.38}{85}{The confusion matrices and results obtained by SVM algorithm regularized with Ridge for $X_{all\_ResNet50}$.\relax }{table.caption.89}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.39}{\ignorespaces The confusion matrices and results obtained by SVM algorithm regularized with Lasso for $X_{all\_ResNet50}$.\relax }}{85}{table.caption.90}\protected@file@percent }
\newlabel{tab:svmlasso_all_results}{{6.39}{85}{The confusion matrices and results obtained by SVM algorithm regularized with Lasso for $X_{all\_ResNet50}$.\relax }{table.caption.90}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.40}{\ignorespaces The confusion matrices and results obtained by LR algorithm for $X_{all\_ResNet50}$.\relax }}{85}{table.caption.91}\protected@file@percent }
\newlabel{tab:lr_all_results}{{6.40}{85}{The confusion matrices and results obtained by LR algorithm for $X_{all\_ResNet50}$.\relax }{table.caption.91}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.41}{\ignorespaces The confusion matrices and results obtained by LR algorithm regularized with Ridge for $X_{all\_ResNet50}$.\relax }}{86}{table.caption.92}\protected@file@percent }
\newlabel{tab:lrridge_all_results}{{6.41}{86}{The confusion matrices and results obtained by LR algorithm regularized with Ridge for $X_{all\_ResNet50}$.\relax }{table.caption.92}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.42}{\ignorespaces The confusion matrices and results obtained by LR algorithm regularized with Lasso for $X_{all\_ResNet50}$.\relax }}{86}{table.caption.93}\protected@file@percent }
\newlabel{tab:lrlasso_all_results}{{6.42}{86}{The confusion matrices and results obtained by LR algorithm regularized with Lasso for $X_{all\_ResNet50}$.\relax }{table.caption.93}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.43}{\ignorespaces The confusion matrices and results obtained by KNN algorithm for $X_{all\_ResNet50}$.\relax }}{86}{table.caption.94}\protected@file@percent }
\newlabel{tab:knn_all_results}{{6.43}{86}{The confusion matrices and results obtained by KNN algorithm for $X_{all\_ResNet50}$.\relax }{table.caption.94}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.44}{\ignorespaces The confusion matrices and results obtained by LDA algorithm for $X_{all\_ResNet50}$.\relax }}{86}{table.caption.95}\protected@file@percent }
\newlabel{tab:lda_all_results}{{6.44}{86}{The confusion matrices and results obtained by LDA algorithm for $X_{all\_ResNet50}$.\relax }{table.caption.95}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.45}{\ignorespaces The confusion matrices and results obtained by LDA algorithm regularized with Lasso for $X_{all\_ResNet50}$.\relax }}{87}{table.caption.96}\protected@file@percent }
\newlabel{tab:ldalasso_all_results}{{6.45}{87}{The confusion matrices and results obtained by LDA algorithm regularized with Lasso for $X_{all\_ResNet50}$.\relax }{table.caption.96}{}}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{7. \bf {CONCLUSIONS AND RECOMMENDATIONS}}{89}{chapter.7}\protected@file@percent }
\newlabel{ch:CH7}{{7}{89}{CONCLUSIONS AND RECOMMENDATIONS}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Future Work}{91}{section.7.1}\protected@file@percent }
\citation{adagrad}
\citation{padam}
\citation{elasticnet_paper}
\citation{ensemble_learning}
\bibstyle{itubib}
\bibdata{tez}
\bibcite{HandMitRingen}{1}
\bibcite{weight_decay_regularization}{2}
\bibcite{dropout_article}{3}
\bibcite{A_novelCNNModel}{4}
\bibcite{AlexNet}{5}
\bibcite{ResNet}{6}
\bibcite{parallel_linear_algebra}{7}
\bibcite{history_coronavirus}{8}
\bibcite{coronavirus_species}{9}
\bibcite{who_pandemic_declaration}{10}
\bibcite{who_qa}{11}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\bf  {\bibnameToC }}{93}{chapter*.97}\protected@file@percent }
\bibcite{covid19_news}{12}
\bibcite{pcr_cleveland_clinic}{13}
\bibcite{covid_vs_pneumonia}{14}
\bibcite{literature_ARDAKANI}{15}
\bibcite{literature_Pathak}{16}
\bibcite{imagenet}{17}
\bibcite{literature_OZTURK}{18}
\bibcite{yolo_darknet}{19}
\bibcite{literature_oh}{20}
\bibcite{literature_elshennawy}{21}
\bibcite{literature_Al-Falluji}{22}
\bibcite{SGD_Momentum}{23}
\bibcite{Adam}{24}
\bibcite{VGG}{25}
\bibcite{lasso_penalty}{26}
\bibcite{svm_original}{27}
\bibcite{support_vector_clustering}{28}
\bibcite{svm_book}{29}
\bibcite{svm_penalty}{30}
\bibcite{scikit-learn}{31}
\bibcite{lr_newton_cg}{32}
\bibcite{lr_lbfgsb}{33}
\bibcite{lr_liblinear}{34}
\bibcite{lr_sag}{35}
\bibcite{lr_saga}{36}
\bibcite{knn_pdf}{37}
\bibcite{kd_tree}{38}
\bibcite{ball_tree}{39}
\bibcite{regularized_lda}{40}
\bibcite{scikit-learn_lda-solvers}{41}
\bibcite{TULIP_package}{42}
\bibcite{adagrad}{43}
\bibcite{padam}{44}
\bibcite{elasticnet_paper}{45}
\bibcite{ensemble_learning}{46}
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\bf {APPENDICES}}{97}{chapter.8}\protected@file@percent }
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\addvspace {0\p@ }}
\@writefile{toc}{\contentsline {section}{APPENDIX A.1 ReadMe File}{99}{chapter.1}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {section}{ISTANBUL TECHNICAL UNIVERSITY - Institute of Science and Technology}{99}{chapter.1}\protected@file@percent }
\newlabel{istanbul-technical-university---institute-of-science-and-technology}{{A}{99}{APPENDIX A.1 ReadMe File}{chapter.1}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {subsection}{Ozan GÜLDALİ}{99}{section*.98}\protected@file@percent }
\newlabel{ozan-guldali}{{A}{99}{APPENDIX A.1 ReadMe File}{section*.98}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {subsection}{Department of Mathematical Engineering - Mathematical Engineering Program}{99}{section*.98}\protected@file@percent }
\newlabel{department-of-mathematical-engineering---mathematical-engineering-program}{{A}{99}{APPENDIX A.1 ReadMe File}{section*.98}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {subsection}{M.Sc. THESIS}{99}{section*.98}\protected@file@percent }
\newlabel{m.sc.-thesis}{{A}{99}{APPENDIX A.1 ReadMe File}{section*.98}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {subsection}{DEEP FEATURE TRANSFER FROM DEEP LEARNING MODELS INTO MACHINE LEARNING ALGORITHMS TO CLASSIFY COVID-19 FROM CHEST X-RAY IMAGES}{99}{section*.98}\protected@file@percent }
\newlabel{deep-feature-transfer-from-deep-learning-models-into-machine-learning-algorithms-to-classify-covid-19-from-chest-x-ray-and-demographic-information}{{A}{99}{APPENDIX A.1 ReadMe File}{section*.98}{}}
\@writefile{toc}{\setcounter {tocdepth}{3}}
\@writefile{toc}{\contentsline {subsubsection}{How to Run}{99}{section*.99}\protected@file@percent }
\newlabel{how-to-run}{{A}{99}{APPENDIX A.1 ReadMe File}{section*.99}{}}
\@writefile{toc}{\setcounter {tocdepth}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Package Versions}{102}{Item.4}\protected@file@percent }
\newlabel{package-versions}{{A}{102}{APPENDIX A.1 ReadMe File}{Item.4}{}}
\@writefile{toc}{\contentsline {section}{APPENDIX A.2 Python Notebook File}{104}{Item.4}\protected@file@percent }
\@writefile{toc}{\addvspace {-10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\bf  {\ozgecmisnameToC }}{107}{Item.4}\protected@file@percent }
